FlinkX基础教程(二)_《FlinkX使用入门及贡献》.Flinkx基础教程_02《FlinkX使用入门及贡献》.308096096

> 2022年8月28日 下午 8:04

> 1小时4分钟33秒

# 关键词

`插件` `日志` `数据源` `服务器` `脚本` `模块` `内存` `字符串` `格式` `指标信息` `断点续传` `配置文件` `数据同步` `本地文件` `输出信息` `编译打包` `运行环境` `快照路径`

# 文字记录

大家晚上好。感谢大家收听观看这一次的 Flink 叉使用入门及贡献。然后我是这次的分享人土豆。接下来我们就开始讲这。本次课程我主要为大家分享就是如何。首先要搭建一个 Flink 叉级的环境，然后的话如何提交任务就是基本的使用，然后以及 Flink 叉它的基本的一些 input format 和 output format 这种 readerreader 插件的一些基本的原理。然后以及怎么去开发一个根据自己的需求去开发一个这样的一个自己的按 reader 这样的一个插件。

 

行。我们开始进入课程的第一章，一共分三章，一个是运行环境的搭建，一个是使用的入门，然后最后一个时开发与贡献。

---

## 01 运行环境搭建

我们开始今天的第一章是 Flink 叉运行环境的搭建，其实搭建还是比较简单的。

### flink

首先我们是准备 Flink 的运行环境 Flink 然后我们需要本次的话是主要针对于是 1.8 的版本的 Flink 叉。然后我们这次需要下载的是一个 1.8 版本的 Flink 然后这边有提供了一个下载地址，还可以点进去，点进去我们可以看到是这样的一个界面，你可以。然后的话你根据你自己的版本，然后选择你对应需要下载的一些包。然后我们这边下的是这个最大的 274 兆的这个，然后 Scala 版本是2.12，我们把它下来，然后上传到服务器上面。

然后第二个是要下载一些，如果你这边是使用那个 hadoop 的话，你这边还需要下载一个 hadoop 的 uber包。有一些同学的话经常发现报一些什么类缺失，很有可能就是因为这个原因造成的。然后这边我也提供了一个链接，我们点进去可以看一下它就是这样的一个 hadoop view 包的一个下载路径。

这边的话需要选择自己对应的 hadoop 版本。我们这边的是 2.7 的 hadoop 然后是flink1.8，然后我们这边的话就点到这个点进去。然后我们也是下载最大的这个是 40 多兆的这样的一个包，这边包我已经提前下好了，这边就不再下载了。

###  flinkx

这边首先是 Flink 的话这边已经下载完成了，然后我们是再下载 Flink 叉， Flink 叉的话直接从 github 上面下载出，下载下来就可以了。下载的话两种，一种是一种是直接 git 克隆，一种是直接下载自己文化，这两种方式是没有任何区别的。然后是下载完，然后我们需要安装驱动，因为有些包像 DB two 达梦 Oracle 这种，它是中央仓库，是没有这些jar包的。然后我们需要去手动去把这些包加到我们的自己的 Maven 仓库里面去。然后这边 Flink 叉是提供了这样的一个脚本的，直接可以执行，我们可以到服务器上面去看一下。这个断开了。

 

这边我是放在 home 目录下可以看一下。这边是我下载下来的一些，一个是 Flink 的 Flink 的包，一个是 Flink 叉的包，我们先进到 Flink 叉。我们可以看到这样的一个目录，这是解压出来的一个目录。然后的话首先我们看到 bin 里面 B 里面是一个是 Flink 叉的一个，这个是启动的脚本，一个下面是安装的安装一些驱动包的一些脚本。然后我们可以执行这个 install just.sh 这边我已经执行了支援，事前已经安装过了，这边就不许不用再演示了。然后然后这些jar包的话，其实如果想自己手动安装的话，它是在点价目录下面可以看看，可以看一下这边就是一些我们提供的一些这边的驱动包，如果你这边中仓库没有可以直接手动安装这一些。然后安装完之后的话，我们需要对那个 Flink 叉进行编译编译打包编译打包就是执行 MN 这样的一个操作，操作后的话我们可以编译完成编译的需要的时间还是比较久的。编译完成后的话，我们可以在 Flink 仓库下面看出可以多了一个 plugins 的这样的一个目录，这个目录的话是存的，就是我们的插件包了。

 

然后在在那个福林叉一点零一零版本的话，这个是称之为 Sync plugins 我们可以看一下编译出来的，这边我已经事先编译好了，所以我们这边现在的话可以直接进去看一下，可以看下这些就是 Flink 叉插件包，可以看出数量还是很多的，知识的种类还是比较丰富。然后这些包插件包是比较多，然后有些可能自己这边用不到，然后可以是手动可以把这些包删掉，也可以，这样的话可以节省一些服务器的资源。当然你也可以直接在 pool 文件里面把它给注释掉，这样的话在编译的过程中的话就不会把这些包进行编译打包了。我们可以看一下 pom 文件。

 

home 文件里面主要是在这个 model 这一块，上面是一个括包，然后这边是一个 launch 模块 test 模块 string 然后下面的话这边就是属于一些离线的模块，像 RDB 的话是一个公共包，就相当于是 JDBC 里支持 JDBC 连接的数据库的一个公共包。然后下面是各种数据数据库数据库的插件包。如果你有些用不到的话，其实可以把它给注释掉。不管注释哪一个的话都是没有影响的。这样的话如果你是用不到这样的话，你打包重新编译打包的话会快很多，然后打包出来的插件包体积也会有所减小。

 

而到这一步的话，整个运行环境就算搭建完成了，是不是感觉还是比较简单的？一共就**下载，然后就打包。这边打包插件包，打，打包完的话接下来就是正式的开始**了。

## 02 使用入门

使用的话首先我们介绍一下flinkX的四种的提交模式。一共是。第一种的话是 local 模式， local 模式的话又称为本地模式。以该模式运行的话，会在本地服务器上面启动一个 Flink 启动并执行一个 Flink 任务。因此本地的话是不需要下载安装 Flink 的，也就是它会启动一个 Flink mini cluster 这样的一个东西。然后使用 local 模式。

 

最大的好处就是轻便快捷。然后当你临时需要将某份数据源的数据同步到另一个数据源时，你无需额外的环境配置，只需要编写任务的脚本便可将轻松的将数据同步过去。不过与此同时，这个模式的话一次性只能运行一个任务，并且在任务运行的过程中是没有办法查看一些指标等信息的，但是在结束后的话还是可以看到这些信息，因此的话是适用于小数据量低频率的任务。然后第二个的话就是就是 Flink 的 style long 模式。 Flink slack normal 又是又是又称为 Flink 集群，需要事先启动 Flink session 而这个模式的话，这个模式的话是可以同时运行多个同步任务，也可以很方便的观察任务的运行状态以及日志信息。但是由于 task manager 持续运行的缘故的话，它是它是不适合长时间多任务的持续提交运行的，因为可能 task manager 的话它内存可能会爆掉。

 

然后第三种的话是三四种的话都是跟 hadoop 集群相关的。一种是 YARN 模式，一种是 yupper 模式。 YARN 模式的话是借助 hadoop 的 YARN 来管理 Flink session 并且可以通过队列的话来隔离不同的按 Flink 筛选适合高频率的任务提交目前我们在生产环境中主要用到的就是这种模式，这种模式还是非常稳定的。然后第四种的话就是 yum yum 模式又称之为 PRO 模式这种模式的话是为每一个任务单独申请一个 session 并且可以自由的配置任务所需要的资源。比如说你这边任务需要的内存是比较多的，然后的话就可以在这边把内存给参数给调大，然后适合资源消耗较大，以及如实时采集间隔轮询这种的话需要长时间稳定运行的任务。

 

然后四种模式到这里就介绍完了。接下来我们就正式的来用一下这四种模式。首先是 local 模式，logo模式。我们首先我这里准备了一个脚本，是一个最简单的一个脚本啦。就是首先这边一个是 stream reader stream reader 这个 stream 插件就是一个构造数据的一个插件。 read 的话这边的话就是用来构造数据。然后这边的 types ID ID 的话就表示是从 1 开 0 开始自动增长每一个 int 类型的一个数据，比如从 01234 这种的。然后下面的话就 type type string 的话就随机 mock 出一个 string 字符串出来。然后下面这边还有一个参数是那个 record count record count 的话，这里的 100 就表示我们只需要构造出 100 条数据。如果这边不填的话，不填的话它是持续不断的构造数据。然后这边也可以是填多个，必须要填这边100，然后逗号再来个100，这表示有两个通道，这下面的话就得配 channel 就得配个 2 了。

 

这这样的话这个文档基于这个参数的详细设计的话是在文档上面都有的，大家伙可以到时候在 github 上面看一看。然后到了下面那个 string writer writer 的话很简单，就一个参数 print 是否把这个数据打印出来。这边是 true true 的话它就打印到控制台，然后记录到日志上面了。所以说这整个任务的话它就表示构造 100 条数据，然后把这个数据输出至控制台。然后这边脚本介绍完了，我们再来看一下命令，命令的话直接是 bin Flink 叉这边就是提交任务的一个命令。 model 模式就是 local 模式，job的话这边就是一个 job 的话就是这边一个 job 就是这样的一个任务，然后配置了任务的绝对路径。然后 plugin root 就是刚才编译打包的 plugins 的一个路径。然后我们把这个命令复制一下。

 

然后我们进到就是这个。对，然后他就直接执行了，我们来看一下他输出的日志。

 

这篇日志信息出来了。

 

看这边 classloader 的话它是表示加载了哪些价包。我们可以看到这边 RDB 模块这边是公共作为公共模块加载进来的，然后这边有加载到。看 string reader writer 一个，还有 string reader。

 

这个环境怎么有点慢。

 

我们把这个任务给停掉。好了。好了好了，刚才可能服务器负载比较大。然后现在好了，我可以看到有输出信息，可以看到这边这里的话就是 string outgroup mat 它所打印的一些信息就是数据的信息。我们可以看它是从一共是 100 条的这边首先这里的话就是表示它的 ID 值，然后下面这中间竖线是一个分割符，然后后面是它 mock 的 string 类型的数据，可以看到一共是到这里一共是 100 条。

 

到这边表示 subs subtask 0 close 表示这边这个任务已经这个已经执行完毕了，接下来它会打印一些指标信息，可以，可以看到这里 number reader 表示读取的数量是 100 条，然后是 number 对 number writer ，在这个地方这边表示 100 条读 100 条写 100 条，把这边任务争杀结束掉了。

 

这边就是一个 local local 模式， local 模式， logo 模式。接下来的话我们看到的是这边是一些看到的，日志信息，然后刚才我们已经看到了，然后还有一个输出的一些相关的指标信息。这边刚才我们也看到了，接下来就是 stand alone 模式。

 

stand alone 的话它需要事先启动一个 Flink session 启动一个 Flink session 然后这边这个任务的话，我们还是用这样的一个简单的任务来。做一个示范。首先我们按照这边的操作步骤进到 Flink B 目录下执行这样的一个命令。

 

然后也是flink1.8，是并我们去执行这样一个启动一个 style long 的集群。

 

可以看到这边输出了这 33 行信息，说明这边 3 long 集群的话已经正式起来了。对了，说一下，就是这边 Flink 的话解压之后解压之后的话就是这样的，目录这边是什么的配置信息都没有，都是没有更改的，也不需要更改。目前的话这样目前它默认的配置信息就已经足够了。然后我们进到它的默认端口。

 

可以看到这样的一个 star alone 的话已经起来了，然后这边这些都是它的默认配置。我看到 task manager 这边他打印了一些日志信息，然后就 manager 这边的话这边打印出这三行信息，然后话这边可以正常访问。然后接下来就是向 Flink 上线 style along 的话去提交任务了。提交任务的话也是这样几个参数，它多了一个多了几个参数。我们一一来看一下。首先还是这个直接 Flink bin Flink 叉这样的一个提交任务的一个脚本。然后的话 model 的话我们这边选用 style logjob 的话还是刚才那样一个，然后 plugin root 也是刚才指定的 plugin 的一个目路径。接下来多了两个参数，一个是 Flink account Flink up 就是你这边 Flink 的配置文件，然后是 Flink live 架它的那个 live 包的一个路径，然后把这个命令也拷一下。

 

然后我们还是到这个目录去执行，我们先把刚才的那个这个输出信息给删掉。

 

你再来看一下，看这边是他的一些提交的信息，这边已经应该已经提交上了，你看任务就已经提交到这来了，我们可以看到这边正在执行 running 然后我们现在就等待待它执行完成。

 

在这个地方我们可以看到它的一些指标信息。这边显示已经读取完成了，这样我们去日志里面 task manager 里面去看一看它所打印的一些信息。看这边的话就是它所打印的跟刚才 local 模式其实打印的日志信息是一样的，就是说它这边的格式是一样的。当然它后面 mock 的数据可能不太一样。看这边是从一开始一直到 100 条这样的一个数据。

 

然后最终也是 close close 我们看一看这个任务这边已经 finish 了，那我们，再看，刷新一下它的日志，可以看他最后把这些资源就卖了多远，这个给关闭两连接。然后我们在这个地方看一下它的指标信息。这里的指标信息跟刚才 local 模式跑的其实指标都是一样的，可以看他写的条数 100 条，读的条数 100 条。具体的指标的话这些指标都有都是什么含义的话，这个是在那个文档上面有说明的，大家可以去看一下。然后 style low 模式这样也就跑完了，是不是也觉得也挺简单的，一点也不复杂，这样就是一个 style alone 模式。

 

最终为这边是日志的一些输出信息在这里，然后是指标信息。然后接下来第三个是 YARN 模式。 YARN 模式的话是需要我们把刚才准备到的这样一个 uber 包给传上去，然后我们先把刚才的 star loan 给停掉。

 

然后 UGC 包就刚才下载要放到那个 live 目录下。对，就是这个包这个包我之前已经放上去了，这个包还是比较大的。然后 pull 模式的话这个样模式的话需要你这边首先把那个 hadoop 的集群都给搭建好，当然我这边已经是搭建好了的，然后这边就启动 Flink session 启动 Flink session 的话进到 Flink 闭目录，执行下面的一个语句。

 

其实它在这个地方的话是有一些区别的，主要区别是在于这个 shift 这样的一个参数。首先我们来看看这样的运行 young 筛选，然后是没有 shift 这样的一个参数的话，它是以这种方式运行的话，是它需要在每个服务器节点的相同路径下都部署 Flink 插件包。然后的话并且 Flink conf.yaml 这样的一个配置文件中的话这样的一个 class load 这样的一个参数的话需要设置成默认的。 On childfirst. 然后后续更新插件包时，所有的节点的话都需要同时更新，否则的话它会报一个什么流水号不一致这样的一个错。然后这种优点是在于筛选启动是比较快的，因为它每个服务器上面都有这样的一个插件包，它就不需要再去上传了。然后下面这样一个杠 shift 这样的一个参数的话，杠 shift 这样一个参数的话它是只需要在提交任务的节点的话，它部署 Flink 叉的插件包。然后并且的话这个这个 CONF Flink conf.yaml 这样的一个文件中的 classloader 这样的一个配置的话需要改成 parent first 这种方式的话，它是在启动的时候，它会把这些插件包全部上传到 hds 然后在其他运行节点的话，任务运行的节点的话，它再从 HDFS 上面拿这些插件包。所以的话它需要上传和下载。这样的话它筛选启动速度会比较慢。

 

改成这个 parent first 的话，因为它是在启动的时候，它会把这些场景包都给加载进来的话加载进来。所以的话你这边必须得指定 parent first 来规避的话一些架构的冲突。因为父类已经父亲 parent class loader 里面已经有了，就必须得先从那个 parent 里面去拿，否则的话它会出现类冲突这样的一个问题。接下来我们先启动一个 Flink session ，这边我以第一种方式进行演示。

 

还是？进到闭目录下。

 

给我们等待它的执行。

 

看这边的话已经已经变成 accepted 状态了。

 

它这边已经显示执行完成了，执行完成了，我们带再到里面去看一看。

 

可以看到这边它就是一个 running 的状态，我们可以点到这个点进去可以看这个界面的话其实跟刚才的 stand alone 其实几乎是完全一样的，只不过它的那个任务的调度方式是交给样来执行。当你刚才如果是使用杠 shift 的方式的话，它会在这个地方把一些插件包，你那个插件包路径下的插件包，它会把它输出到这个信这里来打印到这块。然后你到你通过搜索 Flink 叉的话是可以可以这边是可以找到一些插件包的。然后我们这边是没有以杠 shift 方式，所以现在这里看不到。

 

接下来这边三线已经启动成功了。那么现在的话就开始提交任务了，任务的话还是以刚才那个简单的 string 这个例子为例。然后我们看一下这个命令。首先这几行都是一样的，最终就多了一个 young confyunk of yunk of 的话就是你这边 hadoop 的一些配置， YARN 的配置，我们把这个命令复制一下。然后我们把这个也停掉，也是把刚才那个输出日志先把它给删干净，然后我们来执行。

 

我们可以看一下这边看，可以看到这个任务就已经提交上去了，这个就是它的一个 job ID 我们再到这个来看一看，看任务已经到这儿来了，可以看到这边的 job ID 和这里的是保持一致的。再来看看这边的一个指标信息，指标信息看这里面。这边 59 刚才是写到59，然后现在是变成，也是 100 条数据。

 

好，我们来看看，task manager 这边还没退出，我们赶紧来看一看。你看同样它也是打印到这个地方，这边输出的信息也是一样的。

 

这边任务已经 finish 了。其实样模式的话提交也就也是比较简单的主要的区别。这边主要的区别只是不同任务的话只是一个脚本的区别。好我们返回这里看一下。和 storm alone 模式一样，我们可以在那个 task manager 和 accumulators 中看到相关的日志输出以及指标信息。刚才我们也已经看到了。然后最后的起量模式是一个 young per younger 模式又称 projo 模式， PRO 模式的话也是也是依赖 hadoop 的。然后 hadoop 集群也是，所以它也需要这样的一个 hadoop uver 这样的一个 O 然后我们它是它是直接起了一个 Flink session 所以说我们这边不需要起，然后我们可以直接提交。还是 projob 的话也是通过这样的一个最简单的一个 string 这样的一个脚本 JSON 脚本，用来把命令也复制一下。

 

mini 命令的话我们看一下它这个模式，看一下这个命令一些参数。 model young per 然后 job 也是刚才那个 jobplugin root 这是 Flink 叉插件的一个路径。然后 Flink CONF Flink 的配置。 young CONF hadoop 的配置，然后一个 plugin load mode class path 这个表示就是这边，因为在 hadoop 的集群的目录下面都已经部署了插件。

 

然后我们可以看到这个就是他这一次任务提交的一个 application ID 然后我们再到集群上面去看一看，这个关掉。我看到这个就是我们刚才启动的，然后我们可以点进去可以看到这边就是运行了一个任务了。这边和 style along 和 session 的话其实这边没有多大的区别，这个也是它的指标信息，这边 100 显示已经读完了，读写完成了。然后的话我们可以看看 task manager 的日志。

 

个 manager 它同样也是输出到这个地方，这边是它的一个输出信息。

 

这边已经跑完了，跑完了我们再刷一下它就没了。没了的话这边看日志要看它日志的话一个方式是这个地方看它的日志这边的话只能看到他的那个 joe job manager 的日志这边就是他的一个桌面按钮相关信息。然后如果我们想要看到，因为有些业务信息的话，它都是记录在一个 task manager 里面。如果我们想要看的话，我们只能通过命令去服务器上面看。然后在这个地方我们已经拿到了它的 application ID 然后我们通过命令的方式。

 

这边的话就可以拿到它日志聚合的一些信息。可以看到这边是它输出的打印的一个数据，我们找一找，这边就是它的 task manager.log 了，所以这边压迫模式也就这么简单，也就直接提交上去了，任务也就提交上去，运行成功了。然后这边是看日志，刚才也说到这个 application ID 就是本次 prejava 提交的 application ID 然后是看它的 task manager 的日志，通过这样的一个 yunlocks 杠 application ID 然后加上你任务的刚才日志里面输出的信，输出的 ID 就可以看到这边所有的日志了。然后这边使用四种模式的话就介绍到这的话就介绍完了。模式其实也感觉也还是挺简单的，没有什么复杂的地方。

 

最后的话是一个参数，我们来看一看 Flink 叉这个 shell 里面的话它都有哪一些参数。一个是 mod mod 这边好像写错了，多来一个应该是 mod 然后执行模式，也就是刚才对应的四种模式，local style long per 然后这 job 对应的对应着它 job 的一个绝对路径 job 这个 JSON 然后是 job ID 这个其实是可以指定的，但是我们一般不需要去指定这样一个东西。然后是 plugin rootplan root 是打包后的一个 Flink 叉的插件包的一个路径。然后是 flinkconf 1 就是 Flink 的一个配置文件的路径，一个是 Flink live 架，一个就是它的 live 包的路径，一个是 young CONF 是 hadoop 的配置路径由 screen 指定到样子队列。

 

刚才我们可以看到，其实我们这边其实都是一个 default 都是一个 default default 的队列其实在生产环境上面，它是有各种各样很多个队列的。它比如说 abc 它是可以提交到不同的队列的，方便一些资源的，任务的隔离，然后是 plugin load mode 加载方式，一种是 class pass 一种是 shipfile 然后是 con CONF propes 这样的一个这样的差这样的一个配置。这个配置主要是用来配置 checkpoint 的。这边有这样一个配置，下个供应的一个频率，这个是一个是多长时间？执行一次 checkpoint 主要是这个配置。然后其余其实你还可以在这个里面加上你自己的一个 Flink 的一些配置。比如说你想要指定一下这次的任务的资源的话，task manager 的资源的话是需要多少大的内存，你也可以在这边指定它这里的参数，它会覆盖掉你这边 Flink cup 的这样的一个参数。但是如果你像 style long 和 session young session 这样的一个模式，其实如果你已经启动了的话，它其实是不会去更改的，因为它更改位置更改的。

 

然后最后一个 S 是一个 checkpoint 的一个快照路径，指定它的话是主要是用于断点续传这样一个功能。最后一个是 P 这样一个自定义参数入参，这个是最近新加的一个可以。如果你的脚本里面存在这样一个占位符，比如说写到那个写到 hdms 然后的话里面它是有分区的。你不同分区域的话可能脚本是一样，但是里面就一个分区不一样，你可以在这样这样进行配置，进行一个动态配置了。这样的话到这里，然后整个使用的入门就已经介绍完了。

 

可以看到整个流程其实也是比较全。 zol 主要任务的提交上去任务的提交四种模式，然后一些现有的参数。然后接下来的话是讲开发与贡献，讲的话话如果这边我们自己的话有一个数据，需要做一个某个数据库的数据同步，但是插件里面又没有这个数据库，我们自己去如何去这样开发一个字这样的一个插件。

## 03 开发与贡献

接下来的话就是尝试通过一个从通过一个开发者的角度，尽可能全面的阐述一个 Flink 插件所经历的过程与执行原理。同时消除开发者的困惑，快速上手插件的开发。 Flink 叉是流式的。然后我们从数据流的角度来看， Flink 叉可以理解为不同的数据源的数据流，通过对应的 Flink 叉插件处理，变成了符合 Flink 叉数据规范的数据流。脏数据的处理可以理解为数，脏水流通过污水处理场变成符合标准可以使用的水流，而对不能处理的水流收集起来。

 

插线开发的话，它不需要要关注任务的话是如何调度的，只需要关注以下三个关键的问题。第一个，数据源本身读写数据的正确性。你数据源读你需要正确的读，把这个数据读出来。然后你得正确的把这些数据给写进去。这是第一点。第二点的话是如何合理且正确的使用框架。第三点就是配置文件的规范。主要还是在第一点。第二点。第三点，其实只要符合规范的话，其实没有太大的问题，包括插件开发的规范以及配置文件参数设置的规范。

 

第一个是开发环境。目前 Flink 还是目前主要还是由基于 1.8 来开发的。单机环境不需要安装 Flink 然后我们目前的话是一点八点一，同时我们也推出了一点一零版本的 Flink 叉。接下来的话我们会主要将开发的重点转换转现为一点一零版本，毕竟现在 1.11 也已经出来了。然后的话 JDK 的话是 8 及以上，操作系统的话开发阶段的话是没有区别的。但是在测试的时候 Windows 系统是 star alone young per 模式都提交不了了，你这边的话就只能到服务器上面去上传了。这一点的话是比较有局限性的， Windows 系统比较有局限性的。

 

然后是逻辑执行概念。插件开发者不需要关心太多整个框架如何具体运行，只需要关注数据源的读写以及代码逻辑上是怎么被执行的方法什么时候被调用。而以下概念化对的理解的话对你快速开发是会有很大帮助的。一个是 job 中国的话是 Flink 叉，用以描述一个源头到另一个目的端的同步任务，是 Flink 叉数据同步的最小业务单元。相当于一个你这边配置了一个任务脚本，就相当于是对应的一个 job 然后是 internal internal 这样的话是把 job 拆分得到的最小的执行单元，然后是 input slide 然后这样的数据分片是在进入 in internal 的最小数据流的单元里面，包含了基本的数据信息和统计信息，然后是 input format 和 output format input format 就是读插件的执行单，也是我们主要关心的读取逻辑的实现。然后 output format 是写插件的执行单位，然后也是我们主要关心的写入逻辑的实现，然后任务的执行模式。

 

刚才在第二章已经说到，一共是四种，然后分别都是跟 Flink 这边相对应的 log 模式，对应的单机模式， link low 模式，对应的分布式模式，然后 YARN 模式对应的是 aflink 集群的 YARN 模式，然后是 yumport 就对应 job 在实际开发过程中的话，上述几种模式对插件的编写其实没有过多的影响一般是在本地 logtest 通过其实整个任务逻辑上面其实是没有太大的问题了。但是值得注意的是，样模式的修样模式下的 super file 方式，该模式的话会将 Flink session Flink session 会加载所有的插件，因此可能会出现一些类冲突，遇到类冲突的话解决还是比较头疼的。

 

这种方式的话我们可以是使用 sheet 的插件。将这个插件的话 sheet 打包可以把它给 relocation 掉。这个的话可以有兴趣可以看一下 Maven 的 sheet 插件还是一个挺有用的一个插件的，然后是插件的入口类。插件入口类需要集成那个。

 

data reader 和 data writer 在内部获取 JSON 获取任务 JSON 传来的参数，通过相应的 builder 构建对应的 input format outputphoto format 这样的一个实例。我们可以看下某个 read 对象的内容如下，可以看到，首先是 protodic string 一个参数，然后是一个构造器，构造器的话它是有一个 super 然后下面是一个 read data 的一个方法，其实总共就两个方法，一个构造器，一个 read data 一个方法。接下来我们看详细详细看一下。

 

reader 继承 data data reader 同时重写 reader data 大的方法，在构造函数中获取任务 JSON 构建 input format 所需要的一些参数。具体如下，就是说它这边的话会传一个 data transport config 这样一个对象。这样一个对象的话，它其实是包含了这个整个任务的所有的信息的。你可以 config.get job 就获得它的 job 所有的信息。然后你可以最后最终 get reader 就是获得到整个 reader 对象的一个所有的信息，包括你的参数，你的 read 的名称之类的，然后把它封装成一个 read config 这样的一个对象。 read config 的话你可以从中拿到他的 parent 拿到他们的 parent 集合，然后最终拿到拿取到某一个你所需要的一个参数，通过这样一个 config key 定义它的一个 key 值，最终拿着这样一个 one param 这样一个对象，然后把这个对象给初始化掉。

 

然后以 protection 这边主要以 protection 的方式，主要是用于可能有些会继承这样的一个插件。比如说你这边版本不同，只是一个加包的区别的话，其实这些业务逻辑是没有必要再写一遍的，直接通过继承的方式就可以。然后第二个是重写那个 readdata 的一个方法。首先你要利用一个 builder 这样的 input format builder 这样一个对象，然后把这些参数都设置到 builder 这样的里面去，最终调用 create input 这样的一个方法，返回这样的一个实例来看一下插件。然后就是 data writer 了，data writer 其实跟 reader 确实是一模一样的，只不过一个叫 reader 一个叫 writer 的区别罢了。我可以简单的看一下。

 

其实这边一样，也是一个 days transformer config 然后从 config 里面拿到对应的 writer config 这样的一个参数，然后构建 output format builder 把这些参数都传进去，最终把它给返回，create input 方式把它给返回掉。然后是 input format builder 的设计。 builder 的话其实主要一个里面保存了一个对应的 input format 这样的一个 format 对象。然后就是一些 format 的参数的设计设置，通过 set 方式设进去。最终的话有一个 check format 对于你所需要的一些必要的参数，必须得传来参数。然后的话你在这边做一个校验，如果校验不通过的话，就在这边抛出一个异常，终把任务给终止掉。

 

我们可以看看 input format input format 主要有这些方法，我们来一一看一下。首先是首先它要继承一个 reach improvement 这样的一个父类。首先然后有一些这样一 config open improvement open int 这样的等等这些方法，我们来，一一看看这个方法。

 

首先是 config 方法，它调用位置是， config 方法会在 join manager 里面构建执行计划的时候再和 task manager 初始化，然后初始化并发实例后各调一次，就是说这个方法会调两次，然后用于然后用于配置 task 的实例。然后这个方法里面的话不要在这个方法里面写耗时的逻辑，比如获取连接运行超时，这样的话可能会导致阿卡超时。所以说在这个方法里面，我们一般不在这个方法里面做一些操作，直接留空。

 

然后我们是主要在一些 O open input format 里面做一些操作。这个是 open inputformat 话是在创建分片之后调用。对整个 inputformat 资源做一些初始化。然后下面一个接下来是调用 open internal 这个的话调用位置是在开始读取封面的时候调用，用于打开需要读取的数据源并做一些初始化。这个方法的话必须是可重复调用的，因为同一个并发时里面可能有多个分片多通道。然后下面是 create input 这样的一个方法。在调用位置是构建执行计划时调用。作用是调用此类的逻辑生成数据分片。需要值得注意的是，分片的数量和并发数没有严格的对应关系。同时也不要在这个方法里面做一些耗时的操作，否则的话也会导致阿卡超时。

 

然后是 next record internal 这样的一个方法，它是用来读取每一条数据的。就是说每读到一条数据的话都会调，都会在这个方法里面。调用一次这个方法，它的作用就是返回到下一条的数据记录。然后是 reach end reach end 的话它是在整个它有 Flink 上里面有个 DT input format 方法里面有个 run 在 run 的时候首先它会判断。这样的话是否还有下一条记录？如果有的话它会调用 next record 如果没有的话就说明这边数据已经读取完了，然后做一些操作。它的调用位置是在任务运行时读取每一条数据时调用。然后后面的话就接下来就是一个 close internal 这样的一个方法，用于读取完一个分片后，至少调用一次，作用的话是关闭资源。注意的话，它这边也是可重复调用关闭资源做一些 final 检查，因为程序遇到异常的话可能会直接跳到 close internal 就掉了，就是跳转到这个方法里面。最终最后面是 close input format 但是这个是当所有的切片都执行完成后，调用用于关闭整个 input format 的整个资源。可以看到主要的一些方法的话就是这些方法。然后只需要在每个方法里面做一些合适的操作的话，其实剪整个的 input format 这样的一个逻辑类的话其实也就实现了。

 

然后接下来是 outputoutput format builder 这个和 input format builder 其实是一样的，只不过名称不太一样而已，也是在这边设置一些参数，然后做 check format 这样的一些操作，检查参数是否正确。然后是 autoflowconformat 这边的话是叉的方法就比较少了，一个是 open internal 一个是 write single record 一个是 write multirecords open internet 调用位置是开始写入时调用作用的话用于打开需要写入的数据源做一些初始化。然后 read single record 就是写单条数据。 open internal 之后调用的话开始写单条数据，这样的话就是向数据源里面一条一条写数据。

 

然后 write multiflymultipyraggle 的话就是写多条数据，然后有一个是 benchy interview 这样一个参数的话是决定每次写入多少条。对于执行 write a single record 这样的一个还是 right market ，关键就是 bench interview 这样的一个参数。

 

等于 E 的话是调用写单条，如果是大于 E 的话，它就是写多条。然后同时注意的就是在写多条的时候出现写入异常的话，它就会一条调用写一条的数据，将一条的方法将数据一条一条的写，直到出现那些某条异常的数据的话，它可能会就如果是配置了脏数据的话，它就会把这些脏数据给收集起来。然后 link 叉的数据结构，它直接是延续了 Flink 原生的数据类型 row 然后是任务 JSON JSON 的话这边就是首先是 reader 而 reader 的话它层级一个是 parent parent 就是表示这些这个任务。 reader 任务里面需要哪些参数。然后下面是个 name name 的话表示这个是哪个 reader 就是它的名称 reader 的名称。

 

然后下面是 writer writer 的话也是参数名称，其实整个它的格式的话基本就是基本是类似的。然后如何设计配置参数。而任务配置中 read 和 write 的话， parament 都部分是插件的参数配置参数。插件的配置参数应当遵循以下的原则，一个是图文命名。所有的配置项采用头部命名法，按首字母小写按单词首字母大写。然后正交原则，配置项必须正交功能，没有重复，没有潜规则，而是父类型。

 

合理使用 JSON 的类型，减少无所谓的处理逻辑，减少出错的可能。然后是使用正确的数据类型，比如 BOOL 类型就用 true false 而非使用什么 ES 去 0 等等。然后是合理使用集合类型，比如用数组代替有分隔符的字符串。类似通用的就是同一插件的话，同一类型的插件化尽量它的格式是保持一致的。

 

然后是项目的命名。项目的话，插件 read writer 把需要放在符合插件包命名规则的 read 下。比如 MySQL read 类需要放在 com.dt stack.flink 叉点 mysql.read 包。讲一下插件的命名规则优化应当遵循以下的原则，一个是插件的命名模版 inflink 插件 datasoname 里，如 Flink 叉杠 MySQL 然后是插件模块的命名模版，用 Flink 插件大 source name.read write core 这样的一个格式。然后是插件包的命名模板，一个是 com DT stack 重新插件，它数字点你的包，比如说是点 reader.writer.input.format 然后点什么兼 util 这样的一个。然后最后是插件包 read writer 命名模版的话，例如 MySQL read Redis writer 这样的话，数据源首字母大写，其余全小写。因此类似于这种 rest API write the meta data have 这种框命名的话是是错误的。

 

然后需要改成这样，首字母大写取全小写这样的一个格式。而主要的原因是因为这边是通过反射的话去调用这些的。然后如果你这边是驼峰，反射是通过反这的方式去加载，这些类的话则加载不到，会把 class 漏放的，因此的话是需要保持这种格式。到这里的话，整个分享到就到这里结束了。大家有什么问题吗？

 

大家有什么对有疑问的，可以在这个时候可以问一下。

 

有吗？

 

比如大家对哪里还有一些不清楚的地方，需要问一下的地方，然后后续有什么意见都可以在这个时候可以提出来。

 

好像没有分片怎么弄这个跟你的数据在第一次江博师兄讲解的时候，有提到数据分辨的话是有几种模式的，一个是通过数据库了，通过 mod 的方式，而一种是文件这样的一个方式。你要看你这边的数据源是什么样的类型。比如说如果是 JDBC RDB 的数据库的话，你可以通过 mode 的方式，然后把它进行对于每条的那个数字类型进行 mod 然后 mod 取余具体类型的话，其实在每个不同的插件里面都有类似的参考。比如说如果是 JDBC 你可以参考 JDBC 里面 JDBC input format 里面是如何创建的？这个你可以参考一下具体的那个模块里面是怎么写的，你这边的话可以具体的看一看你这边是到底是要什么样的一个数据库。 local 模式可以运行在什么配置的机器上？负载有多大？ local 模式的话其实对机器的要求不大，你至少内存有一个 G 足就已经够了。负载的话只是这边在创建那个 task manager 的话可能会有一些。

 

然后这边 local 的话其实是临时使用还是比较好的，不建议长期的话多次多任务的持续提交。然后是 JSON 配置，后续会提供一些插件帮忙构建么？这个是可能这边 JSON 配置的话其实是在其实在 github 上面是每个插件它都提供了的，我们可以看一下每个插件都有相应的模板的，教你这边帮忙构建是也就是说这边可以看到每个插件比如说我们随便点到一个插件，它都是有好几个配置模板，你可以根据你自己的需要去选取。去修。去拿取对应的模板进行相应的修改。你可以看到这边还是比较全面的。

 

job.json 就可以用 last API 而不是使用本地文件。目前的话还是只能使用原本地文件进行提交的。你可以。目前还是不支持 rest API 去远程提交一个任务的。你这边的话要用可以自己只能写一个项目，然后通过 rust API 去提交到上面，然后他在服务器上面本地去。执行这样的一个shell。

 

如果任务运行在 YARN 上面是在哪儿配置 YARN 集群的地址。这个是在这个是在那个 YARN 的配置文件里面配置的。我们可以看到在启动的时候，我们在启动的时候传参的时候是需要传到样的一些配置文件的。我们可以看到这个地方我们可以看样模式的话它是需要查那个 young CONF 它在这个里面去读取的样子集群的地址的。也就是说你这边搭好之后，只需要传这个文件的路径就可以了，不需要做额外的一些配置。

 

字帧字段是字符串目前自增字段。字符串的话是不支持的，因为它需要去比较。比如说我们这边 MySQL 这种 RDB 的话它是通过大于号小于号这种的话进行比较的，你字符串比较的话可能会有问题，所以建议还是使用时间数值类型的这种字符串不这种类型的参这种类型的字段 Cronjob 可以使用传达 hdmi 上的 JSON 目前是不行的，这个的话是读取，因为它是读取的是本地文件，在 launch 模块的话它是没有 HDFS 上面的，没有读到这样的一个功能。当然你也可以，其实本地文件和 HDFS 上的文件的话其实差别不是很大，这样的这边的话你是可以直接稍微改一下 lash 模块的读取文件的里面的一个读取方式。这样的话其实这样的话就可以很简单的实现你这样的一个需求了。

 

对，我们目前的话是只支持一个本地文件的，如果是一个接口的 JSON 目前还是。没有这样的一个功能的。后续你们有需求的话我们可以看。后续考虑可以开发一个这样的一个功能。

 

潘森写的任务，这个目前是不支持的。 Python 写的任务目前是不支持的。断点续传后续会支持吗？目前断点续传其实功能是已经有了，现在其实是已经支持断点续传这样的一个功能了，在后续的课程中的话活跃会继续给大家分享那个断点续传这样一个功能？具体是如何实现的？具体是如何？完成一个断点续查这样一个功能后面的话是有这样的一个课程安排给大家分享的。

 

断点续传其实这边简单说一下。断点续传其实主要是一些数据量比较大的任务，它运行时间比较久，然后可能到中途的话，因为某些意外情况的话，它任务挂掉了，挂掉的话如果从头开始的话会占会耗费比较久的时间。然后因此的话就有断点续传这样功能。

 

其实这个断点的话是基于 Flink 的 checkpoint 这样一个机制，就是每当触发 checkpoint 的时候，我们会把当前读取到的值保存，保存，保存下来。比如说我们当前读取到的 ID 比如说是一百万两百万某一个点，然后的话这个它保存，把这个通过 checkpoint 把这个 checkpoint 保存到 HDFS HDFS 上面，然后在日志里面会输出这个 HDFS 这样一个路径，然后也会输出一些指标等信息。然后这边任务失败了的话，我们去我们可以到那个 hm 上面可以拿到它的 checkpoint 最新的一个 checkpoint 的值。这个当然可以是 HDFS 也可以是本地路径，目前的话我们操作的话都是基于 HDFS 上面的。然后的话刚才也可以看到通过一个 S 的这样一个参数，它就是传了这样的一个路径。我们可以看一看这样一个 checkpoint 的快照路径。我们把这个参数传进去之后的话，它会读取 checkpoint 里面所保存的一些指标信息。比如说我们当前读取到 100 万，然后的话我们我们在这边任务执行的时候，我们会构建一个过滤条件，比如说 ID 大于 100 万，这样的话我们就可以过滤掉前面 100 万已经读到的数据，从而从后面的数就开始继续读取，然后输出这样的话，我们就实现了这样一个断点续传的这样一个功能。大致就是这样。

 